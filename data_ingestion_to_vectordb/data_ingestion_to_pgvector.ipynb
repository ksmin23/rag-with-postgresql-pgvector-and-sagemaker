{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93179240-9c5f-4ba6-a1c7-3a981624f794",
   "metadata": {},
   "source": [
    "# Ingest massive amounts of data to a Vector DB (Amazon Aurora Postgresql with pgvector)\n",
    "**_Use of Amazon Aurora Postgresql as a vector database for storing embeddings_**\n",
    "\n",
    "This notebook works well with the `Data Science 3.0` kernel on a SageMaker Studio `ml.t3.medium` instance.\n",
    "\n",
    "Here is a list of packages that are used in this notebook.\n",
    "```\n",
    "!pip list | grep -E -w \"sagemaker|ipython-sql|langchain|psycopg2|pgvector|numpy|sh\"\n",
    "----------------------------------------------------------------------------------------\n",
    "ipython-sql                          0.5.0\n",
    "langchain                            0.1.0\n",
    "langchain-community                  0.0.20\n",
    "langchain-core                       0.1.52\n",
    "langchain-text-splitters             0.0.1\n",
    "numpy                                1.24.3\n",
    "pgvector                             0.2.0\n",
    "psycopg2-binary                      2.9.6\n",
    "sagemaker                            2.155.0\n",
    "sagemaker-studio-image-build         0.6.0\n",
    "sh                                   2.0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aae52c-cd7a-4637-a07d-9c0131dc7d0a",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e64f84-b7ac-427d-b5a8-cf98b430be9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain==0.1.0 --quiet\n",
    "!pip install -U ipython-sql==0.5.0 --quiet\n",
    "!pip install -U psycopg2-binary==2.9.6 --quiet\n",
    "!pip install -U pgvector==0.2.0 --quiet\n",
    "!pip install -U sh==2.0.4 --quiet\n",
    "!pip install -U sagemaker-studio-image-build==0.6.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88757ba-7ae1-4efb-9c02-ab17ec22e79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list | grep -E -w \"sagemaker|ipython-sql|langchain|psycopg2|pgvector|numpy|sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017bc3f-e507-4f0c-b640-ea774c5ea9c8",
   "metadata": {},
   "source": [
    "## Step 2: Download the data from the web and upload to S3\n",
    "\n",
    "In this step we use `wget` to crawl a Python documentation style website data. All files other than `html`, `txt` and `md` are removed. **This data download would take a few minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b8c14-0ffc-4090-adf1-c2a8a1bdebaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEBSITE = \"https://sagemaker.readthedocs.io/en/stable/\"\n",
    "DOMAIN = \"sagemaker.readthedocs.io\"\n",
    "DATA_DIR = \"docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb232ee-6b62-4718-9104-345fe7978703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./scripts/get_data.py --website {WEBSITE} --domain {DOMAIN} --output-dir {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1fbb8-583a-4c41-a831-715e4250ff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "aws_region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c127969-4abc-4a31-8829-c00bee321a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_OS_INDEX_HINT_FILE = \"_create_index_hint\"\n",
    "app_name = 'llm-app-rag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25217f27-4995-4da5-8fc4-b1b9533185b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dummy file called _create_index to provide a hint for Postgresql index creation\n",
    "# this is needed for Sagemaker Processing Job when there are multiple instance nodes\n",
    "# all running the same code for data ingestion but only one node needs to create the index\n",
    "!touch {DATA_DIR}/{CREATE_OS_INDEX_HINT_FILE}\n",
    "\n",
    "# upload this data to S3, to be used when we run the Sagemaker Processing Job\n",
    "!aws s3 cp --recursive {DATA_DIR}/ s3://{bucket}/{app_name}/{DOMAIN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15667be7-43b5-4954-95e0-885c9173f82c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Load data into Aurora Postgresql with pgvector \n",
    "\n",
    "We now have a working script that is able to ingest data into an Aurora Postgresql. But for this to work for massive amounts of data we need to scale up the processing by running this code in a distributed fashion. We will do this using Sagemkaer Processing Job. This involves the following steps:\n",
    "\n",
    "1. Create a custom container in which we will install the `langchain`, `psycopg2` and `pgvector` packges and then upload this container image to Amazon Elastic Container Registry (ECR).\n",
    "2. Use the Sagemaker `ScriptProcessor` class to create a Sagemaker Processing job that will run on multiple nodes.\n",
    "    - The data files available in S3 are automatically distributed across in the Sagemaker Processing Job instances by setting `s3_data_distribution_type='ShardedByS3Key'` as part of the `ProcessingInput` provided to the processing job.\n",
    "    - Each node processes a subset of the files and this brings down the overall time required to ingest the data into the Aurora Postgresql.\n",
    "    - Each node also uses Python `multiprocessing` to internally also parallelize the file processing. Thus, **there are two levels of parallelization happening, one at the cluster level where individual nodes are distributing the work (files) amongst themselves and another at the node level where the files in a node are also split between multiple processes running on the node**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d2938-994f-432d-8c50-92269f22f4b8",
   "metadata": {},
   "source": [
    "### Create custom container\n",
    "\n",
    "We will now create a container locally and push the container image to ECR. **The container creation process takes about 1 minute**.\n",
    "\n",
    "1. The container include all the Python packages we need i.e. `langchain`, `psycopg2`, `pgvector`, `sagemaker` and `beautifulsoup4`.\n",
    "2. The container also includes the `credentials.py` script for retrieving credentials from Secrets Manager and `sm_helper.py` for helping to create SageMaker endpoint classes that langchain uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01848619-8b49-48da-8cbf-c9cbbd8d1e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOCKER_IMAGE = \"load-data-pgvector-custom\"\n",
    "DOCKER_IMAGE_TAG = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd70a7-8f61-477c-a8ef-82c0b5cd7821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ./container && sm-docker build . --repository {DOCKER_IMAGE}:{DOCKER_IMAGE_TAG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089064e-2099-4226-82c1-c7c406203c49",
   "metadata": {},
   "source": [
    "### Create and run the Sagemaker Processing Job\n",
    "\n",
    "Now we will run the Sagemaker Processing Job to ingest the data into Aurora Postgresql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b236a7-b5d2-494e-a3e3-baec94adc3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d9a38-ae89-44af-83db-657dd7e851d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "import boto3\n",
    "\n",
    "\n",
    "def get_cfn_outputs(stack_name: str, region_name: str = 'us-east-1') -> List:\n",
    "    cfn = boto3.client('cloudformation', region_name=region_name)\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stack_name)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def get_secret_name(stack_name: str, region_name: str = 'us-east-1'):\n",
    "    cf_client = boto3.client('cloudformation', region_name=region_name)\n",
    "    response = cf_client.describe_stacks(StackName=stack_name)\n",
    "    outputs = response[\"Stacks\"][0][\"Outputs\"]\n",
    "\n",
    "    secrets = [e for e in outputs if e['ExportName'] == 'VectorDBSecret'][0]\n",
    "    secret_name = secrets['OutputValue']\n",
    "    return secret_name\n",
    "\n",
    "def get_secret(secret_name: str, region_name: str = 'us-east-1'):\n",
    "    client = boto3.client('secretsmanager', region_name=region_name)\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "\n",
    "    return json.loads(secret)\n",
    "\n",
    "def get_db_subnet_ids(stack_name: str, region_name: str = 'us-east-1'):\n",
    "    cfn_outputs = get_cfn_outputs(stack_name, region_name)\n",
    "    db_cluster_id = cfn_outputs['DBClusterId']\n",
    "\n",
    "    rds_client = boto3.client('rds', region_name=region_name)\n",
    "    db_cluster_info = rds_client.describe_db_clusters(DBClusterIdentifier=db_cluster_id)\n",
    "    db_subnet_group_name = db_cluster_info['DBClusters'][0]['DBSubnetGroup']\n",
    "    db_subnet_info = rds_client.describe_db_subnet_groups(DBSubnetGroupName=db_subnet_group_name)\n",
    "    db_subnet_ids = [e['SubnetIdentifier'] for e in db_subnet_info['DBSubnetGroups'][0]['Subnets']]\n",
    "\n",
    "    return db_subnet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d74214",
   "metadata": {},
   "source": [
    "##### Create the pgvector extension on your Aurora PostgreSQL database (DB) cluster\n",
    "\n",
    "[pgvector](https://github.com/pgvector/pgvector) is an open-source extension for PostgreSQL that adds the ability to store and search over ML-generated vector embeddings. pgvector provides different capabilities that let you identify both exact and approximate nearest neighbors. It’s designed to work seamlessly with other PostgreSQL features, including indexing and querying. Using ChatGPT and other LLM tooling often requires storing the output of these systems, i.e., vector embeddings, in a permanent storage system for retrieval at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71514284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "CFN_STACK_NAME = \"RAGPgVectorStack\" # name of CloudFormation stack\n",
    "\n",
    "secret_name = get_secret_name(CFN_STACK_NAME)\n",
    "secret = get_secret(secret_name)\n",
    "\n",
    "db_username = secret['username']\n",
    "db_password = urllib.parse.quote_plus(secret['password'])\n",
    "db_port = secret['port']\n",
    "db_host = secret['host']\n",
    "\n",
    "driver = 'psycopg2'\n",
    "\n",
    "connection_string = f\"postgresql+{driver}://{db_username}:{db_password}@{db_host}:{db_port}/\"\n",
    "connection_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $connection_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed914bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE EXTENSION IF NOT EXISTS vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT typname\n",
    "FROM pg_type\n",
    "WHERE typname = 'vector';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cfadf",
   "metadata": {},
   "source": [
    "##### Load the embeddings and LLM into Aurora PostgreSQL DB cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c8f74-faa9-4c4a-a6b6-de1b5699f955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFN_STACK_NAME = 'EmbeddingEndpointStack'\n",
    "\n",
    "cfn_stack_outputs = get_cfn_outputs(CFN_STACK_NAME, aws_region)\n",
    "embeddings_model_endpoint_name = cfn_stack_outputs['EmbeddingEndpointName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29861415-9ad9-44bc-b2f2-ca8bf61dd40c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFN_STACK_NAME = \"RAGPgVectorStack\"\n",
    "\n",
    "pgvector_secret_id = get_secret_name(CFN_STACK_NAME, aws_region)\n",
    "pgvector_collection_name = 'llm_rag_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749f0fa-2df2-4289-9033-f429aac6e2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d12e82-239c-4844-9e2c-73bd25c49167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE_FOR_DOC_SPLIT = 500\n",
    "CHUNK_OVERLAP_FOR_DOC_SPLIT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64826d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_subnet_ids = get_db_subnet_ids('RAGPgVectorStack', aws_region)\n",
    "db_client_security_group_id = get_cfn_outputs('RAGPgVectorStack', aws_region)['DBClientSecurityGroupId']\n",
    "sagemaker_domain_security_group_id = get_cfn_outputs('RAGSageMakerStudioStack', aws_region)['DomainSecurityGroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87843222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "\n",
    "# For more information, see https://docs.aws.amazon.com/sagemaker/latest/dg/process-vpc.html\n",
    "network_config = NetworkConfig(security_group_ids=[sagemaker_domain_security_group_id,\n",
    "                                                   db_client_security_group_id],\n",
    "                               subnets=db_subnet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7373c-e80f-4d1a-a8dc-0dc79fb28e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ScriptProcessor\n",
    ")\n",
    "\n",
    "# setup the parameters for the job\n",
    "base_job_name = f\"{app_name}-job\"\n",
    "tags = [{\"Key\": \"data\", \"Value\": \"embeddings-for-llm-apps\"}]\n",
    "\n",
    "# use the custom container we just created\n",
    "image_uri = f\"{account_id}.dkr.ecr.{aws_region}.amazonaws.com/{DOCKER_IMAGE}:{DOCKER_IMAGE_TAG}\"\n",
    "\n",
    "# instance type and count determined via trial and error: how much overall processing time\n",
    "# and what compute cost works best for your use-case\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 3\n",
    "logger.info(f\"base_job_name={base_job_name}, tags={tags}, image_uri={image_uri}, instance_type={instance_type}, instance_count={instance_count}\")\n",
    "\n",
    "# setup the ScriptProcessor with the above parameters\n",
    "processor = ScriptProcessor(base_job_name=base_job_name,\n",
    "                            image_uri=image_uri,\n",
    "                            role=aws_role,\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=instance_count,\n",
    "                            command=[\"python3\"],\n",
    "                            tags=tags,\n",
    "                            network_config=network_config)\n",
    "\n",
    "# setup input from S3, note the ShardedByS3Key, this ensures that\n",
    "# each instance gets a random and equal subset of the files in S3.\n",
    "inputs = [ProcessingInput(source=f\"s3://{bucket}/{app_name}/{DOMAIN}\",\n",
    "                          destination='/opt/ml/processing/input_data',\n",
    "                          s3_data_distribution_type='ShardedByS3Key',\n",
    "                          s3_data_type='S3Prefix')]\n",
    "\n",
    "\n",
    "logger.info(f\"creating an pgvector collection with name={pgvector_collection_name}\")\n",
    "\n",
    "# ready to run the processing job\n",
    "st = time.time()\n",
    "processor.run(code=\"container/load_data_into_pgvector.py\",\n",
    "              inputs=inputs,\n",
    "              outputs=[],\n",
    "              arguments=[\"--pgvector-secretid\", pgvector_secret_id,\n",
    "                         \"--pgvector-collection-name\", pgvector_collection_name,\n",
    "                         \"--aws-region\", aws_region,\n",
    "                         \"--embeddings-model-endpoint-name\", embeddings_model_endpoint_name,\n",
    "                         \"--chunk-size-for-doc-split\", str(CHUNK_SIZE_FOR_DOC_SPLIT),\n",
    "                         \"--chunk-overlap-for-doc-split\", str(CHUNK_OVERLAP_FOR_DOC_SPLIT),\n",
    "                         \"--input-data-dir\", \"/opt/ml/processing/input_data\",\n",
    "                         \"--create-index-hint-file\", CREATE_OS_INDEX_HINT_FILE,\n",
    "                         \"--process-count\", \"2\"])\n",
    "\n",
    "time_taken = time.time() - st\n",
    "logger.info(f\"processing job completed, total time taken={time_taken}s\")\n",
    "\n",
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "logger.info(preprocessing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e444161-262e-44e5-ad31-e490a763be4e",
   "metadata": {},
   "source": [
    "## Step 4: Do a similarity search for user input to documents (embeddings) in Aurora Postgresql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a7292-8bdb-4d11-a23d-130a4a039cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from langchain.vectorstores import PGVector\n",
    "\n",
    "from container.credentials import get_credentials\n",
    "from container.sm_helper import create_sagemaker_embeddings_from_js_model\n",
    "\n",
    "\n",
    "secret = get_credentials(pgvector_secret_id, aws_region)\n",
    "\n",
    "db_username = secret['username']\n",
    "db_password = urllib.parse.quote_plus(secret['password'])\n",
    "db_port = secret['port']\n",
    "db_host = secret['host']\n",
    "\n",
    "connection_string = PGVector.connection_string_from_db_params(\n",
    "    driver='psycopg2',\n",
    "    user=db_username,\n",
    "    password=db_password,\n",
    "    host=db_host,\n",
    "    port=db_port,\n",
    "    database=''\n",
    ")\n",
    "\n",
    "docsearch = PGVector.from_existing_index(\n",
    "                                   embedding=create_sagemaker_embeddings_from_js_model(embeddings_model_endpoint_name,\n",
    "                                                                                       aws_region),\n",
    "                                   collection_name=pgvector_collection_name,\n",
    "                                   connection_string=connection_string)\n",
    "\n",
    "q = \"Which XGBoost versions does SageMaker support?\"\n",
    "docs = docsearch.similarity_search(q, k=3)\n",
    "for doc in docs:\n",
    "    logger.info(\"----------\")\n",
    "    logger.info(f\"content=\\\"{doc.page_content}\\\",\\nmetadata=\\\"{doc.metadata}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29eae5-c463-4153-9167-e4628c74d13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "To avoid incurring future charges, delete the resources. You can do this by deleting the CloudFormation template used to create the IAM role and SageMaker notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3fe8-bb71-4e22-a551-2475eb2d16b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "In this notebook we were able to see how to use LLMs deployed on a SageMaker Endpoint to generate embeddings and then ingest those embeddings into Aurora Postgresql and finally do a similarity search for user input to the documents (embeddings) stored in Aurora Postgresql. We used langchain as an abstraction layer to talk to both the SageMaker Endpoint as well as Aurora Postgresql."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53386268-0cf9-4a37-b3d0-711fba1e5585",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332274c-586d-4cf9-838f-ea7e0cbe6c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from container.sm_helper import create_sagemaker_embeddings_from_js_model\n",
    "\n",
    "CFN_STACK_NAME = 'EmbeddingEndpointStack'\n",
    "cfn_stack_outputs = get_cfn_outputs(CFN_STACK_NAME, aws_region)\n",
    "embeddings_model_endpoint_name = cfn_stack_outputs['EmbeddingEndpointName']\n",
    "\n",
    "embeddings = create_sagemaker_embeddings_from_js_model(embeddings_model_endpoint_name, aws_region)\n",
    "\n",
    "text = \"This is a sample query.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "print(query_result)\n",
    "print(f\"length: {len(query_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd881bab",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  * [Leverage pgvector and Amazon Aurora PostgreSQL for Natural Language Processing, Chatbots and Sentiment Analysis](https://aws.amazon.com/blogs/database/leverage-pgvector-and-amazon-aurora-postgresql-for-natural-language-processing-chatbots-and-sentiment-analysis/)\n",
    "  * [Building AI-powered search in PostgreSQL using Amazon SageMaker and pgvector](https://aws.amazon.com/blogs/database/building-ai-powered-search-in-postgresql-using-amazon-sagemaker-and-pgvector/)\n",
    "  * [Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/)\n",
    "  * [Give SageMaker Processing Jobs Access to Resources in Your Amazon VPC](https://docs.aws.amazon.com/sagemaker/latest/dg/process-vpc.html)\n",
    "    * **Configure the VPC Security Group**\n",
    "      * In distributed processing, you must allow communication between the different containers in the same processing job. To do that, configure a rule for your security group that allows inbound connections between members of the same security group.\n",
    "  * [How can I troubleshoot the InternalServerError response on Amazon SageMaker? - AWS re:Post](https://repost.aws/knowledge-center/sagemaker-http-500-internal-server-error)\n",
    "  * [LangChain](https://python.langchain.com/docs/get_started/introduction.html) - A framework for developing applications powered by language models."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:123456789012:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
